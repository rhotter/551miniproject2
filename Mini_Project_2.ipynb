{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m79a_ixHKSNa"
   },
   "source": [
    "# Mini Project 2: IMDB Sentiment Analysis\n",
    "\n",
    "February 22, 2019\n",
    "\n",
    "Akshal Aniche, Jacob Sanz-Robinson, Raphael Hotter\n",
    "\n",
    "COMP 551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-pKZV1tOrb5"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM8MJ94vOvfK"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfiIMynrNxs1"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l97x0eiVOomj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pos_train_files = os.listdir('data/train/pos/')\n",
    "neg_train_files = os.listdir('data/train/neg/')\n",
    "test_files = os.listdir('data/test/')\n",
    "\n",
    "pos_words = 'opinion-lexicon-English/positive-words.txt'\n",
    "neg_words = 'opinion-lexicon-English/negative-words.txt'\n",
    "\n",
    "\n",
    "# Remove .DS_Store files\n",
    "while '.DS_Store' in pos_train_files:\n",
    "  pos_train_files.remove('.DS_Store')\n",
    "while '.DS_Store' in neg_train_files:\n",
    "  neg_train_files.remove('.DS_Store')\n",
    "while '.DS_Store' in test_files:\n",
    "  test_files.remove('.DS_Store')\n",
    "\n",
    "test_files.sort(key=lambda x : int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cGRD8HCOM_O"
   },
   "outputs": [],
   "source": [
    "# Reads data into 2 arrays\n",
    "train_text, test_raw = [], []\n",
    "\n",
    "for file in pos_train_files:\n",
    "  with open('data/train/pos/{}'.format(file), 'r') as f:\n",
    "    train_text.append(f.read().lower())\n",
    "\n",
    "for file in neg_train_files:\n",
    "  with open('data/train/neg/{}'.format(file), 'r') as f:\n",
    "    train_text.append(f.read().lower())\n",
    "\n",
    "for file in test_files:\n",
    "  with open('data/test/{}'.format(file), 'r') as f:\n",
    "    test_raw.append(f.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xt2U3HyfMzD3"
   },
   "outputs": [],
   "source": [
    "# Training set Y vector\n",
    "pos_goal = np.ones((12500))\n",
    "neg_goal = np.zeros((12500))\n",
    "Y_train = np.append(pos_goal, neg_goal, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read lexicon of sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_words = []\n",
    "\n",
    "with open(pos_words, 'r', encoding=\"latin-1\") as f:\n",
    "    for line in f:\n",
    "        if (line[0] != ';'):\n",
    "            sent_words.append(line.strip())\n",
    "\n",
    "with open(neg_words, 'r', encoding=\"latin-1\") as f:\n",
    "    for line in f:\n",
    "        if (line[0] != ';'):\n",
    "            sent_words.append(line.strip())\n",
    "            \n",
    "# print(sent_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processer preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "bin_vec = CountVectorizer(binary=True)\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "bigram_vec = CountVectorizer(ngram_range=(1,1))\n",
    "sentc_vec = CountVectorizer(binary=True)\n",
    "sentt_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize        \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "#Lemmatizer integrated with the tokenizer \n",
    "wnl = WordNetLemmatizer()\n",
    "class LemmaTokenizer(object):\n",
    "    def __call__(self, text):\n",
    "        return [wnl.lemmatize(t) for t in word_tokenize(sent_tokenize(text))]\n",
    "\n",
    "sentt_vec = TfidfVectorizer(tokenizer=LemmaTokenizer())\n",
    "tokenizer=LemmaTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ykKGbYlmMohC",
    "outputId": "76a1cea3-ec72-43b6-b959-a24875ee8b29"
   },
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "bin_vec.fit(train_text)\n",
    "\n",
    "# Create the feautre matrices \n",
    "train_bin = bin_vec.transform(train_text)\n",
    "test_bin = bin_vec.transform(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monograms and Bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "bigram_vec.fit(train_text)\n",
    "\n",
    "# Create the feautre matrices \n",
    "train_bigram = bigram_vec.transform(train_text)\n",
    "test_bigram = bigram_vec.transform(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "tfidf_vec.fit(train_text)\n",
    "\n",
    "# Create the feautre matrices \n",
    "train_tfidf = tfidf_vec.fit_transform(train_text)\n",
    "test_tfidf = tfidf_vec.transform(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf with only sentiment lexicon, Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "sentt_vec.fit(sent_words)\n",
    "\n",
    "# Create the feautre matrices \n",
    "train_s_tfidf = sentt_vec.transform(train_text)\n",
    "test_s_tfidf = sentt_vec.transform(test_raw)\n",
    "\n",
    "normalizer_tran = Normalizer().fit(X=train_s_tfidf)\n",
    "X_train_sn = normalizer_tran.transform(train_s_tfidf)\n",
    "X_test_sn = normalizer_tran.transform(test_s_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def process (X_train, X_test, p, var):\n",
    "    poly = PolynomialFeatures(p)\n",
    "    pca = TruncatedSVD(n_components = 10000)\n",
    "    normalizer_tranformer = Normalizer().fit(X_train)\n",
    "    X_train = normalizer_tranformer.transform(X_train)\n",
    "    X_test= normalizer_tranformer.transform(X_test)\n",
    "\n",
    "\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    '''\n",
    "    poly.fit(X=X_train)\n",
    "    X_train = poly.transform(X_train)\n",
    "    X_test = poly.transform(X_test)\n",
    "    \n",
    "    pca.fit(X=X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    '''\n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_normalized, X_test_normalized) = process(train_tfidf, test_tfidf, 2, 0.95)\n",
    "print (X_train_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer_tranformer = Normalizer().fit(X=train_tfidf)\n",
    "X_train_normalized = normalizer_tranformer.transform(train_tfidf)\n",
    "X_test_normalized = normalizer_tranformer.transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NaiveBayesScratch():\n",
    "    \"\"\"Bernouli Naive Bayes\"\"\"\n",
    "    def train(self, X_train, Y_train): #bow = bag of words\n",
    "#         X_train = X_train.toarray()\n",
    "        Y_train = Y_train.reshape(len(Y_train),1)\n",
    "        num_pos = np.sum(Y_train)\n",
    "        num_neg = len(Y_train) - num_pos\n",
    "        \n",
    "        self.theta_1_ = num_pos/float(len(Y_train))\n",
    "        self.theta_j1_ = (X_train*Y_train).sum(axis=0) # sum along training examples\n",
    "        self.theta_j0_ = (X_train*(1-Y_train)).sum(axis=0)\n",
    "        \n",
    "        # Laplace smoothing\n",
    "        self.theta_j1_ = (self.theta_j1_ + 1)/(float(num_pos) + 2)\n",
    "        self.theta_j0_ = (self.theta_j0_ + 1)/(float(num_neg) + 2)\n",
    "        \n",
    "        # Prepare for predictions\n",
    "        self.predict_theta_1_ = math.log(self.theta_1_/(1-self.theta_1_))\n",
    "\n",
    "        self.predict_pos_ = np.log(self.theta_j1_/self.theta_j0_)\n",
    "        self.predict_pos_ = self.predict_pos_.reshape(len(self.predict_pos_),1)\n",
    "        \n",
    "        self.predict_neg_ = np.log((1-self.theta_j1_)/(1-self.theta_j0_))\n",
    "        self.predict_neg_ = self.predict_neg_.reshape(len(self.predict_neg_),1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "#         X = X.toarray()\n",
    "        pos = np.dot(X, self.predict_pos_)\n",
    "        neg = np.dot(1-X, self.predict_neg_)\n",
    "        delta_predictions = self.predict_theta_1_ + pos + neg\n",
    "        binary_predictions = delta_predictions > 0\n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fd9d2e76ebf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnaive_bayes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNaiveBayesScratch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnaive_bayes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_bin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-0b4a9e2a9973>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, Y_train)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"\"\"Bernouli Naive Bayes\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#bow = bag of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m         \u001b[0m_sparsetools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_todense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "naive_bayes = NaiveBayesScratch()\n",
    "naive_bayes.train(train_bin, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_X_test = train_bin\n",
    "mini_Y_test = Y_train\n",
    "mini_Y_test = mini_Y_test.reshape(mini_Y_test.shape[0],1)\n",
    "\n",
    "predictions = naive_bayes.predict(mini_X_test)\n",
    "# print(predictions==mini_Y_test)\n",
    "score1 = np.sum(predictions==mini_Y_test, axis=0)/float(mini_Y_test.shape[0])\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test standard Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.858634345373815, 0.842913716548662, 0.8471138845553822]\n",
      "Scores Mean: 0.8495539821592865\n"
     ]
    }
   ],
   "source": [
    "# Define train(), evaluate() functions\n",
    "import random\n",
    "import math\n",
    "\n",
    "def k_cross_validate(X_train, Y_train, k):\n",
    "    \"\"\"\n",
    "    X_train: n x m array\n",
    "    Y_train: n x 1 array\n",
    "    k: number of folds\n",
    "    \"\"\"\n",
    "    indeces = random.sample(range(Y_train.shape[0]), Y_train.shape[0])\n",
    "    step = int(X_train.shape[0]/k)\n",
    "    scores = []\n",
    "    for k_fold in range(k):\n",
    "        k_validate_indeces = indeces[k_fold*step:(k_fold+1)*step]\n",
    "        k_train_indeces = [i for i in range(X_train.shape[0]) if i not in k_validate_indeces]\n",
    "        \n",
    "        b = NaiveBayesScratch()\n",
    "        b.train(X_train[k_train_indeces,:], Y_train[k_train_indeces,:])\n",
    "        predictions = b.predict(X_train[k_validate_indeces,:])\n",
    "        score = np.sum(predictions==Y_train[k_validate_indeces,:]) / float(Y_train[k_validate_indeces,:].shape[0])\n",
    "        scores.append(score)\n",
    "    mean = np.array(scores).mean()\n",
    "    print(\"Scores: {}\".format(scores))\n",
    "    print(\"Scores Mean: {}\".format(mean))\n",
    "\n",
    "    \n",
    "formatted_Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "k_cross_validate(train_bin.toarray(), formatted_Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation function for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Function to cross validate a scikitlearn model\n",
    "def crossvalidate(model, X_train, Y_train, fold) :\n",
    "    scores = cross_val_score(model, X_train, Y_train, cv=fold)\n",
    "    mean = scores.mean()\n",
    "    print(\"Scores: {}\".format(scores))\n",
    "    print(\"Scores Mean: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (to make sure it worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.85493161 0.84581234 0.8524964 ]\n",
      "Scores Mean: 0.8510801133028897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bayes_clf = BernoulliNB()\n",
    "crossvalidate(bayes_clf, train_bin, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.86741061 0.85805136 0.85633701]\n",
      "Scores Mean: 0.8605996589883947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bayes_clf = MultinomialNB()\n",
    "crossvalidate(bayes_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.88840893 0.88348932 0.8799808 ]\n",
      "Scores Mean: 0.8839596816892191\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "regr_clf = linear_model.LogisticRegression()\n",
    "crossvalidate(regr_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.70410367 0.70074394 0.70199232]\n",
      "Scores Mean: 0.7022799769873428\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "dec_clf = tree.DecisionTreeClassifier()\n",
    "crossvalidate(dec_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder if we can play with the parameters more "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.89464843 0.89188865 0.88886222]\n",
      "Scores Mean: 0.8917997649962367\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Stacking Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 18029 features per sample; expecting 73626",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-34cdcf1a378b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayes_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0my1_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0my2_validate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbayes_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 18029 features per sample; expecting 73626"
     ]
    }
   ],
   "source": [
    "# test_tfidf = tfidf_vec.transform(test_raw)\n",
    "\n",
    "validate_indeces = random.sample(range(Y_train.shape[0]), 1000)\n",
    "train_indeces = [i for i in range(len(train_text)) if i not in validate_indeces]\n",
    "\n",
    "train_text1 = [train_text[i] for i in train_indeces]\n",
    "validate_text1 = [train_text[i] for i in validate_indeces]\n",
    "\n",
    "tfidf_vec.fit(train_text1)\n",
    "train_tfidf = tfidf_vec.fit_transform(train_text1)\n",
    "validate_tfidf = tfidf_vec.fit_transform(validate_text1)\n",
    "\n",
    "Y_train1 = Y_train[train_indeces]\n",
    "\n",
    "svm_clf = svm.LinearSVC()\n",
    "regr_clf = linear_model.LogisticRegression()\n",
    "bayes_clf = MultinomialNB()\n",
    "\n",
    "svm_clf.fit(train_tfidf, Y_train_rand)\n",
    "bayes_clf.fit(train_tfidf, Y_train_rand)\n",
    "y1 = svm_clf.predict(train_tfidf)\n",
    "y2 = bayes_clf.predict(train_tfidf)\n",
    "\n",
    "y1_validate = svm_clf.predict(validate_tfidf)\n",
    "y2_validate = bayes_clf.predict(validate_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((np.array(y1).reshape(len(y1),1),np.array(y2).reshape(len(y2),1)),axis=1)\n",
    "y_validate = np.concatenate((np.array(y1_validate).reshape(len(y1_validate),1),np.array(y2_validate).reshape(len(y2_validate),1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr_clf = linear_model.LogisticRegression()\n",
    "regr_clf.fit(y,Y_train_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 17568 features per sample; expecting 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-7d91071c926d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregr_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidate_indeces\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 17568 features per sample; expecting 2"
     ]
    }
   ],
   "source": [
    "np.sum(regr_clf.predict(y_validate)==Y_train[validate_indeces])/np.shape(Y_train)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mini-Project-2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

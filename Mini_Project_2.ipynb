{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m79a_ixHKSNa"
   },
   "source": [
    "# Mini Project 2: IMDB Sentiment Analysis\n",
    "\n",
    "February 22, 2019\n",
    "\n",
    "Akshal Aniche, Raphael Hotter, Jacob Sanz-Robinson\n",
    "\n",
    "COMP 551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-pKZV1tOrb5"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM8MJ94vOvfK"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfiIMynrNxs1"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l97x0eiVOomj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pos_train_files = os.listdir('data/train/pos/')\n",
    "neg_train_files = os.listdir('data/train/neg/')\n",
    "test_files = os.listdir('data/test/')\n",
    "\n",
    "pos_words = 'opinion-lexicon-English/positive-words.txt'\n",
    "neg_words = 'opinion-lexicon-English/negative-words.txt'\n",
    "\n",
    "\n",
    "# Remove .DS_Store files\n",
    "while '.DS_Store' in pos_train_files:\n",
    "  pos_train_files.remove('.DS_Store')\n",
    "while '.DS_Store' in neg_train_files:\n",
    "  neg_train_files.remove('.DS_Store')\n",
    "while '.DS_Store' in test_files:\n",
    "  test_files.remove('.DS_Store')\n",
    "\n",
    "test_files.sort(key=lambda x : int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1cGRD8HCOM_O"
   },
   "outputs": [],
   "source": [
    "# Reads data into 2 arrays\n",
    "train_text, test_raw = [], []\n",
    "\n",
    "for file in pos_train_files:\n",
    "  with open('data/train/pos/{}'.format(file), 'r') as f:\n",
    "    train_text.append(f.read().lower())\n",
    "\n",
    "for file in neg_train_files:\n",
    "  with open('data/train/neg/{}'.format(file), 'r') as f:\n",
    "    train_text.append(f.read().lower())\n",
    "\n",
    "for file in test_files:\n",
    "  with open('data/test/{}'.format(file), 'r') as f:\n",
    "    test_raw.append(f.read().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xt2U3HyfMzD3"
   },
   "outputs": [],
   "source": [
    "# Training set Y vector\n",
    "pos_goal = np.ones((12500))\n",
    "neg_goal = np.zeros((12500))\n",
    "Y_train = np.append(pos_goal, neg_goal, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read lexicon of sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_words = []\n",
    "\n",
    "with open(pos_words, 'r', encoding=\"latin-1\") as f:\n",
    "    for line in f:\n",
    "        if (line[0] != ';'):\n",
    "            sent_words.append(line.strip())\n",
    "\n",
    "with open(neg_words, 'r', encoding=\"latin-1\") as f:\n",
    "    for line in f:\n",
    "        if (line[0] != ';'):\n",
    "            sent_words.append(line.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processer preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize, pos_tag     \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_pos(word):\n",
    "    #Map POS tag to first character lemmatize() accepts\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Lemmatizer integrated with the tokenizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        sent = [s for s in sent_tokenize(text)]\n",
    "        words = []\n",
    "        for s in sent:\n",
    "            words = words + word_tokenize(s)\n",
    "        return [self.wnl.lemmatize(t, get_pos(t)) for t in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "bin_vec = CountVectorizer(binary=True)\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "bigram_vec = TfidfVectorizer(ngram_range=(1,2))\n",
    "sentc_vec = CountVectorizer(binary=True)\n",
    "sentt_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "ykKGbYlmMohC",
    "outputId": "76a1cea3-ec72-43b6-b959-a24875ee8b29"
   },
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "bin_vec.fit(train_text)\n",
    "\n",
    "# Create the feautre matrices \n",
    "train_bin = bin_vec.transform(train_text)\n",
    "test_bin = bin_vec.transform(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "tfidf_vec.fit(train_text)\n",
    "\n",
    "# Create the feature matrices \n",
    "train_tfidf = tfidf_vec.fit_transform(train_text)\n",
    "test_tfidf = tfidf_vec.transform(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "bigram_vec.fit(train_text)\n",
    "\n",
    "# Create the feature matrices \n",
    "train_bigram = bigram_vec.fit_transform(train_text)\n",
    "test_bigram = bigram_vec.transform(test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer_tran = Normalizer().fit(X=train_bigram)\n",
    "X_train_sn = normalizer_tran.transform(train_bigram)\n",
    "X_test_sn = normalizer_tran.transform(test_bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf with only sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and build the vocabulary\n",
    "sentt_vec.fit(sent_words)\n",
    "\n",
    "# Create the feautre matrices \n",
    "train_s_tfidf = sentt_vec.transform(train_text)\n",
    "test_s_tfidf = sentt_vec.transform(test_raw)\n",
    "\n",
    "# normalizer_tran = Normalizer().fit(X=train_s_tfidf)\n",
    "# X_train_sn = normalizer_tran.transform(train_s_tfidf)\n",
    "# X_test_sn = normalizer_tran.transform(test_s_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "bigram_vec = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "combined_features = FeatureUnion([(\"bigram_vec\", bigram_vec), (\"tfidf\", tfidf_vec)])\n",
    "X = combined_features.fit(train_text).transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components = 500)\n",
    "pca.fit(X)\n",
    "X_train = pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, PolynomialFeatures\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def process (X_train, X_test, p, var):\n",
    "    poly = PolynomialFeatures(p)\n",
    "    pca = TruncatedSVD(n_components = 10000)\n",
    "    normalizer_tranformer = Normalizer().fit(X_train)\n",
    "    X_train = normalizer_tranformer.transform(X_train)\n",
    "    X_test= normalizer_tranformer.transform(X_test)\n",
    "\n",
    "\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    '''\n",
    "    poly.fit(X=X_train)\n",
    "    X_train = poly.transform(X_train)\n",
    "    X_test = poly.transform(X_test)\n",
    "    \n",
    "    pca.fit(X=X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    '''\n",
    "    return (X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_bigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "pca = TruncatedSVD(n_components = 500)\n",
    "pca.fit(train_tfidf)\n",
    "X_train = pca.transform(train_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components = 100)\n",
    "pca.fit(train_bigram)\n",
    "X_train_2 = pca.transform(train_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_train)\n",
    "X_poly = poly.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly.fit(X_train_2)\n",
    "X_poly_2 = poly.transform(X_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer_tranformer = Normalizer().fit(X=train_tfidf)\n",
    "X_train_normalized = normalizer_tranformer.transform(train_tfidf)\n",
    "X_test_normalized = normalizer_tranformer.transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer_tranformer = Normalizer().fit(X=train_bigram)\n",
    "X_train_normalized_2 = normalizer_tranformer.transform(train_bigram)\n",
    "X_test_normalized = normalizer_tranformer.transform(test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NaiveBayesScratch():\n",
    "    \"\"\"Bernouli Naive Bayes\"\"\"\n",
    "    def train(self, X_train, Y_train): #bow = bag of words\n",
    "#         X_train = X_train.toarray()\n",
    "        Y_train = Y_train.reshape(len(Y_train),1)\n",
    "        num_pos = np.sum(Y_train)\n",
    "        num_neg = len(Y_train) - num_pos\n",
    "        \n",
    "        self.theta_1_ = num_pos/float(len(Y_train))\n",
    "        self.theta_j1_ = (X_train*Y_train).sum(axis=0) # sum along training examples\n",
    "        self.theta_j0_ = (X_train*(1-Y_train)).sum(axis=0)\n",
    "        \n",
    "        # Laplace smoothing\n",
    "        self.theta_j1_ = (self.theta_j1_ + 1)/(float(num_pos) + 2)\n",
    "        self.theta_j0_ = (self.theta_j0_ + 1)/(float(num_neg) + 2)\n",
    "        \n",
    "        # Prepare for predictions\n",
    "        self.predict_theta_1_ = math.log(self.theta_1_/(1-self.theta_1_))\n",
    "\n",
    "        self.predict_pos_ = np.log(self.theta_j1_/self.theta_j0_)\n",
    "        self.predict_pos_ = self.predict_pos_.reshape(len(self.predict_pos_),1)\n",
    "        \n",
    "        self.predict_neg_ = np.log((1-self.theta_j1_)/(1-self.theta_j0_))\n",
    "        self.predict_neg_ = self.predict_neg_.reshape(len(self.predict_neg_),1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "#         X = X.toarray()\n",
    "        pos = np.dot(X, self.predict_pos_)\n",
    "        neg = np.dot(1-X, self.predict_neg_)\n",
    "        delta_predictions = self.predict_theta_1_ + pos + neg\n",
    "        binary_predictions = delta_predictions > 0\n",
    "        return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = NaiveBayesScratch()\n",
    "naive_bayes.train(train_bin, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_X_test = train_bin\n",
    "mini_Y_test = Y_train\n",
    "mini_Y_test = mini_Y_test.reshape(mini_Y_test.shape[0],1)\n",
    "\n",
    "predictions = naive_bayes.predict(mini_X_test)\n",
    "# print(predictions==mini_Y_test)\n",
    "score1 = np.sum(predictions==mini_Y_test, axis=0)/float(mini_Y_test.shape[0])\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test standard Naive Bayes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train(), evaluate() functions\n",
    "import random\n",
    "import math\n",
    "\n",
    "def k_cross_validate(X_train, Y_train, k):\n",
    "    \"\"\"\n",
    "    X_train: n x m array\n",
    "    Y_train: n x 1 array\n",
    "    k: number of folds\n",
    "    \"\"\"\n",
    "    indeces = random.sample(range(Y_train.shape[0]), Y_train.shape[0])\n",
    "    step = int(X_train.shape[0]/k)\n",
    "    scores = []\n",
    "    for k_fold in range(k):\n",
    "        k_validate_indeces = indeces[k_fold*step:(k_fold+1)*step]\n",
    "        k_train_indeces = [i for i in range(X_train.shape[0]) if i not in k_validate_indeces]\n",
    "        \n",
    "        b = NaiveBayesScratch()\n",
    "        b.train(X_train[k_train_indeces,:], Y_train[k_train_indeces,:])\n",
    "        predictions = b.predict(X_train[k_validate_indeces,:])\n",
    "        score = np.sum(predictions==Y_train[k_validate_indeces,:]) / float(Y_train[k_validate_indeces,:].shape[0])\n",
    "        scores.append(score)\n",
    "    mean = np.array(scores).mean()\n",
    "    print(\"Scores: {}\".format(scores))\n",
    "    print(\"Scores Mean: {}\".format(mean))\n",
    "\n",
    "    \n",
    "formatted_Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "k_cross_validate(train_bin.toarray(), formatted_Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation function for sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Function to cross validate a scikitlearn model\n",
    "def crossvalidate(model, X_train, Y_train, fold) :\n",
    "    scores = cross_val_score(model, X_train, Y_train, cv=fold)\n",
    "    mean = scores.mean()\n",
    "    print(\"Scores: {}\".format(scores))\n",
    "    print(\"Scores Mean: {}\".format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes (to make sure it worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bayes_clf = BernoulliNB()\n",
    "crossvalidate(bayes_clf, train_bin, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "bayes_clf = MultinomialNB()\n",
    "crossvalidate(bayes_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr_clf = linear_model.LogisticRegression(solver = 'lbfgs')\n",
    "crossvalidate(regr_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr_clf = linear_model.LogisticRegression(solver = 'lbfgs')\n",
    "crossvalidate(regr_clf, train_bigram, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "dec_clf = tree.DecisionTreeClassifier()\n",
    "crossvalidate(dec_clf, train_s_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, train_bigram_nm, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random and Extra Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier()\n",
    "crossvalidate(clf2, train_bigram, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier()\n",
    "crossvalidate(clf, train_bigram, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regr_clf = linear_model.LogisticRegression()\n",
    "crossvalidate(regr_clf, X, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple TFIDF features \n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, train_tfidf, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF PCA to 100\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X_train, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF + PCA + NORMALIZED\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X_train_normalized, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF + PCA + POLYNOMIAL DEGREE 2\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X_poly, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple TFIDF features including 2-grams\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, train_bigram, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF PCA to 100 bigrams\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X_train_2, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF + PCA + NORMALIZED bigrams\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X_train_normalized_2, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TFIDF + PCA + POLYNOMIAL DEGREE 2 bigrams\n",
    "from sklearn import svm\n",
    "svm_clf = svm.LinearSVC()\n",
    "crossvalidate(svm_clf, X_poly_2, Y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods — Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf1 = LogisticRegression(solver = 'lbfgs')\n",
    "# clf2 = MultinomialNB()\n",
    "clf3 = LinearSVC()\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('svm', clf3)], voting='hard')\n",
    "# crossvalidate(eclf, X, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.89788817 0.89488841 0.89474316]\n",
      "Scores Mean: 0.8958399122597317\n"
     ]
    }
   ],
   "source": [
    "crossvalidate(eclf, train_bigram, Y_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_bigram, Y_train, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer_tranformer = Normalizer().fit(X=X_train)\n",
    "X_train_normalized = normalizer_tranformer.transform(X_train)\n",
    "X_test_normalized = normalizer_tranformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)), ('svm', LinearS...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))],\n",
       "         flatten_transform=None, n_jobs=None, voting='hard', weights=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.91      0.90      2472\n",
      "         1.0       0.91      0.90      0.90      2528\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = eclf.predict(X_test_normalized)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Stacking Method - Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import linear_model\n",
    "\n",
    "# test_tfidf = tfidf_vec.transform(test_raw)\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "validate_indeces = random.sample(range(Y_train.shape[0]), 1000)\n",
    "train_indeces = [i for i in range(len(train_text)) if i not in validate_indeces]\n",
    "\n",
    "train_text1 = [train_text[i] for i in train_indeces]\n",
    "validate_text1 = [train_text[i] for i in validate_indeces]\n",
    "\n",
    "tfidf_vec.fit(train_text1)\n",
    "train_tfidf = tfidf_vec.transform(train_text1)\n",
    "validate_tfidf = tfidf_vec.transform(validate_text1)\n",
    "\n",
    "Y_train1 = Y_train[train_indeces]\n",
    "\n",
    "svm_clf = svm.LinearSVC()\n",
    "regr_clf = linear_model.LogisticRegression()\n",
    "bayes_clf = MultinomialNB()\n",
    "\n",
    "svm_clf.fit(train_tfidf, Y_train1)\n",
    "bayes_clf.fit(train_tfidf, Y_train1)\n",
    "y1 = svm_clf.predict(train_tfidf)\n",
    "y2 = bayes_clf.predict_proba(train_tfidf)\n",
    "\n",
    "y1_validate = svm_clf.predict(validate_tfidf)\n",
    "y2_validate = bayes_clf.predict_proba(validate_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.array(y1).reshape(len(y1),1)\n",
    "y2 = np.array(y2[:,1]).reshape(len(y2),1)\n",
    "y = np.concatenate((y1,y2,y1*y2),axis=1)\n",
    "\n",
    "y1_validate = np.array(y1_validate)\n",
    "y2_validate = np.array(y2_validate)[:,1]\n",
    "y1_validate = y1_validate.reshape(y1_validate.shape[0],1)\n",
    "y2_validate = y2_validate.reshape(y2_validate.shape[0],1)\n",
    "\n",
    "# y1_reshaped = np.array(y1_validate).reshape(len(y1_validate),1)\n",
    "# y2_reshaped = np.array(y2_validate).reshape(len(y2_validate),1)\n",
    "\n",
    "y_validate = np.concatenate((y1_validate,y2_validate),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_clf = linear_model.LogisticRegression()\n",
    "regr_clf.fit(y, Y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(regr_clf.predict(y_validate)) - Y_train[validate_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((np.array(regr_clf.predict(y_validate))-Y_train[validate_indeces])==0)/np.shape(Y_train[validate_indeces])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_test):\n",
    "    Y_test = model.predict(X_test)\n",
    "    return Y_test\n",
    "\n",
    "def write_prediction(model, X_test):\n",
    "    Y_test = (model.predict(X_test)).tolist()\n",
    "    with open(\"submission.csv\", \"w+\") as f:\n",
    "        f.write(\"Id,Category\\n\")\n",
    "        for i in range(25000):\n",
    "            s = \"\" + str(i) + ',' + str(int(Y_test[i])) + \"\\n\"\n",
    "            f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf.fit(train_bigram, Y_train)\n",
    "write_prediction(eclf, test_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = predict(eclf, test_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mini-Project-2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
